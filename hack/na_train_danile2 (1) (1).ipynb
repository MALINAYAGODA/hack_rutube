{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OWrIm_ntehui"
   },
   "outputs": [],
   "source": [
    "!pip install av==11.0.0 decord sentence_transformers faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aUSSUrN0bOwW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import InputExample, losses, SentenceTransformer\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "import faiss\n",
    "from decord import VideoReader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yzoY2O_Uc3j5"
   },
   "outputs": [],
   "source": [
    "with open('/home/jovyan/lost+found/results_2.json', 'r', encoding='utf-8') as f:\n",
    "    data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "QyXvP4wCNI2s"
   },
   "outputs": [],
   "source": [
    "train_data_cat = pd.read_csv(\"/home/jovyan/lost+found/train_data_categories.csv\")\n",
    "IAB_tags = pd.read_csv(\"/home/jovyan/lost+found/IAB_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Zpf3C3tmR1NZ"
   },
   "outputs": [],
   "source": [
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    if seg_len - converted_len <= 0:\n",
    "        start_idx = 0\n",
    "        end_idx = seg_len\n",
    "    else:\n",
    "        end_idx = np.random.randint(converted_len, seg_len)\n",
    "        start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx - 1, num=clip_len)\n",
    "    indices = np.clip(indices, 0, seg_len - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "9e2ac8470f5b4e75ad78254670fb7d13",
      "69ee207e54f84718bf801a2298ee0724",
      "8cbc19b0ee374d1fb49973db0e697d11",
      "253e38d324674a038945336da4806006",
      "a76a2326b719446fb0ac4d807e2dce78",
      "4de140fee82242e8893073b0ace3a862",
      "1ddf7fca3d04406da5abf1ceb1196ecd",
      "f7da282a6c8d43fc93de4e4a853b989c"
     ]
    },
    "id": "2pthdTa7S27s",
    "outputId": "e5d86286-c933-4c08-9dbb-c8949edd4d7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import XCLIPProcessor, XCLIPVisionModel\n",
    "\n",
    "model_name = \"microsoft/xclip-base-patch16-zero-shot\" # microsoft/xclip-base-patch32\n",
    "\n",
    "processor = XCLIPProcessor.from_pretrained(model_name)\n",
    "video_model = XCLIPVisionModel.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3NM6FGH4X-Ue"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0., attn_head_dim=None, out_dim=None):\n",
    "        super().__init__()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        assert all_head_dim == dim\n",
    "\n",
    "        self.q = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.k = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, all_head_dim, bias=False)\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.k_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.k_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, k=None, v=None):\n",
    "        B, N, C = x.shape\n",
    "        N_k = k.shape[1]\n",
    "        N_v = v.shape[1]\n",
    "\n",
    "        q_bias, k_bias, v_bias = None, None, None\n",
    "        if self.q_bias is not None:\n",
    "            q_bias = self.q_bias\n",
    "            k_bias = self.k_bias\n",
    "            v_bias = self.v_bias\n",
    "\n",
    "        q = F.linear(input=x, weight=self.q.weight, bias=q_bias)\n",
    "        q = q.reshape(B, N, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)  # (B, N_head, N_q, dim)\n",
    "\n",
    "        k = F.linear(input=k, weight=self.k.weight, bias=k_bias)\n",
    "        k = k.reshape(B, N_k, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)\n",
    "\n",
    "        v = F.linear(input=v, weight=self.v.weight, bias=v_bias)\n",
    "        v = v.reshape(B, N_v, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (B, N_head, N_q, N_k)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "WSU78eA7nGwI"
   },
   "outputs": [],
   "source": [
    "data = train_data_cat\n",
    "taxonomy = IAB_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6a332c6768ce482387f10129529187d8",
      "c565309629a7461dbd2b8d034820b943",
      "7653a0bfaaa34b5a9840e169b4b32a69",
      "149fefe2d743481da58c825216e09d1e",
      "c3f0c2e9550e451ab9c2efab9b77fe76",
      "ca50f77d47f14bcdaf5445048e7426f2",
      "68b487220e6f4a6fa6797c0324356d1a",
      "1a3220aef5ed44da9562cbd3319f5dca",
      "cc38bf6b60c94524b5f6fa40bcfe5233",
      "97a81df00e1f45aea402958bfa3a3619"
     ]
    },
    "id": "H38TNAG5iHhR",
    "outputId": "d6c7a999-ddfd-4f20-cd14-09eeb5fe364d"
   },
   "outputs": [],
   "source": [
    "text_model = SentenceTransformer('intfloat/multilingual-e5-large-instruct').to('cuda')\n",
    "dim = 1024 # размер вектора эмбеддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9B0Am4NwQPc-"
   },
   "outputs": [],
   "source": [
    "audio_df = pd.DataFrame(list(data2.items()), columns=['filename', 'transcription'])\n",
    "audio_df['filename'] = audio_df['filename'].apply(lambda l: l.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwTlKFSK41Hl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Hhk0uutf41Hl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = '/home/jovyan/lost+found/videos_2/'\n",
    "\n",
    "# Получаем список всех файлов и папок в директории\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Фильтруем, оставляя только файлы (если нужно)\n",
    "files = [f[:-4] for f in files if os.path.isfile(os.path.join(directory, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "3DzYwrMJ41Hm",
    "outputId": "d8e7b567-2e1b-443d-dcaa-09f603dea14b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1049, 2)\n",
      "(934, 2)\n"
     ]
    }
   ],
   "source": [
    "print(audio_df.shape)\n",
    "is_cool = [i in files for i in audio_df.filename.tolist()]\n",
    "audio_df = audio_df[is_cool]\n",
    "print(audio_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "hvOtyoWG41Hm",
    "outputId": "547941ca-2c63-44a5-d680-71e2afe9778a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1049, 4)\n",
      "(934, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "is_cool = [i in files for i in data.video_id.tolist()]\n",
    "data = data[is_cool]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "rrINjKUxSZqJ"
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data.reset_index(drop=True), audio_df.reset_index(drop=True), how='left', left_on='video_id', right_on='filename')\n",
    "data = data.drop('filename', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "Vm45Q_S_XDrX",
    "outputId": "519f28c3-5eff-4fc3-86db-90547947806a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>4354a1ad8bf75f42466420f4b52dcbcd</td>\n",
       "      <td>Артмеханика. Концерт группы Диктофон.</td>\n",
       "      <td>Концерт группы Диктофон.</td>\n",
       "      <td>Массовая культура, Карьера, События и достопри...</td>\n",
       "      <td>Woo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>a8033ba9f69362b009d852b2a9d76749</td>\n",
       "      <td>Три лошадиные силы I Выпуск №14</td>\n",
       "      <td>В этом выпуске парни помогают Деду Морозу разв...</td>\n",
       "      <td>Транспорт, Массовая культура</td>\n",
       "      <td>Дома, что ли?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             video_id                                  title  \\\n",
       "222  4354a1ad8bf75f42466420f4b52dcbcd  Артмеханика. Концерт группы Диктофон.   \n",
       "502  a8033ba9f69362b009d852b2a9d76749        Три лошадиные силы I Выпуск №14   \n",
       "\n",
       "                                           description  \\\n",
       "222                           Концерт группы Диктофон.   \n",
       "502  В этом выпуске парни помогают Деду Морозу разв...   \n",
       "\n",
       "                                                  tags  transcription  \n",
       "222  Массовая культура, Карьера, События и достопри...           Woo!  \n",
       "502                       Транспорт, Массовая культура  Дома, что ли?  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJbOdp0K41Hm"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "U6mTsiTtOkKl"
   },
   "outputs": [],
   "source": [
    "audio_model = SentenceTransformer('intfloat/multilingual-e5-large-instruct').to('cuda')\n",
    "dim = 1024 # размер вектора эмбеддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "0RruAImEdl1c"
   },
   "outputs": [],
   "source": [
    "template = [(isinstance(i, str)) for i in  data[\"tags\"].tolist()]\n",
    "data = data[template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "8fgLX5jVGfbE"
   },
   "outputs": [],
   "source": [
    "title_description = []\n",
    "tag_list = []\n",
    "video_ids = []\n",
    "audio_text = []\n",
    "\n",
    "for i in data.iterrows():\n",
    "  tags = i[1].tags.split(\", \")\n",
    "  title = i[1].title\n",
    "  description = i[1].description\n",
    "  video_id = i[1].video_id\n",
    "  for j in range(len(tags)):\n",
    "    tag_list.append(tags[j])\n",
    "    title_description.append(\"Title: \" + title + \". Description: \" + description)\n",
    "    video_ids.append(video_id)\n",
    "    audio_text.append(i[1].transcription)\n",
    "\n",
    "df_work = pd.DataFrame({\"video_id\": video_ids, \"title_description\": title_description,\n",
    "                        \"audio_text\": audio_text, \"tag_list\": tag_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "W4LQnCKw1UFl"
   },
   "outputs": [],
   "source": [
    "#df_work['title_description_vector'] = df_work['title_description'].apply(lambda l: text_model.encode(l, convert_to_tensor=True).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Gxv3nQyOXshr"
   },
   "outputs": [],
   "source": [
    "#df_work['audio_text_vector'] = df_work['audio_text'].apply(lambda l: audio_model.encode(l, convert_to_tensor=True).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "1Tena4TKodtV"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_work, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2-hPdCLIYBks"
   },
   "outputs": [],
   "source": [
    "class FusionEmbedModel(nn.Module):\n",
    "  def __init__(self, video_emb_size, text_emb_size, audio_text_emb_size,\n",
    "               out_emb_size=768, num_heads=8):\n",
    "    super().__init__()\n",
    "    self.description_title_video_cross_attn = CrossAttention(\n",
    "            dim=text_emb_size,\n",
    "            num_heads=num_heads,\n",
    "            out_dim=text_emb_size\n",
    "        )\n",
    "\n",
    "    self.audio_text_video_cross_attn = CrossAttention(\n",
    "        \n",
    "        dim=audio_text_emb_size,\n",
    "        num_heads=num_heads,\n",
    "        out_dim=audio_text_emb_size\n",
    "    )\n",
    "\n",
    "    total_emb_size = text_emb_size + audio_text_emb_size\n",
    "    self.linear_video_proj = nn.Linear(video_emb_size, text_emb_size)\n",
    "    self.out_proj = nn.Linear(total_emb_size, out_emb_size)\n",
    "\n",
    "  def forward(self, description_title_emb, audio_text_emb, video_emb):\n",
    "        # description_title_emb: (B, N_desc, C_text)\n",
    "        # audio_text_emb: (B, N_audio, C_audio_text)\n",
    "        # video_emb: (B, N_video, C_video)\n",
    "\n",
    "        video_emb = self.linear_video_proj(video_emb)\n",
    "\n",
    "        description_title_video_attn = self.description_title_video_cross_attn(\n",
    "            x=description_title_emb, k=video_emb, v=video_emb\n",
    "        )  # (B, N_desc, C_text)\n",
    "\n",
    "        audio_text_video_attn = self.audio_text_video_cross_attn(\n",
    "            x=audio_text_emb, k=video_emb, v=video_emb\n",
    "        )  # (B, N_audio, C_audio_text)\n",
    "\n",
    "        # average pooling -> bottleneck (we can try different strategies)\n",
    "        description_title_attn_pooled = description_title_video_attn.mean(dim=1)  # (B, C_text)\n",
    "        audio_text_attn_pooled = audio_text_video_attn.mean(dim=1)    # (B, C_audio_text)\n",
    "\n",
    "        cat_embs = torch.cat([\n",
    "            description_title_attn_pooled,\n",
    "            audio_text_attn_pooled\n",
    "        ], dim=-1)  # (B, total_emb_size)\n",
    "\n",
    "        universal_emb = self.out_proj(cat_embs)  # (B, out_emb_size)\n",
    "        return universal_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "iBB1ywPboq-4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "video_model.eval()\n",
    "video_embs_dict = dict()\n",
    "video_embs = []\n",
    "for i in train_df.iterrows():\n",
    "  # видео эмбеддинги засовываем в train_df в новую колонку\n",
    "  video_path = \"/home/jovyan/lost+found/videos_2/\" + str(i[1].video_id) + \".mp4\" # поменять на директорию, где хранятся видео, типо video_id = \"folder/\" + video_id\n",
    "  if video_path in video_embs_dict:\n",
    "    video_embs.append(video_embs_dict[video_path])\n",
    "  else:\n",
    "    vr = VideoReader(video_path)\n",
    "    seg_len = len(vr)\n",
    "    clip_len = 32 # для конкретной модели (microsoft/xclip-base-patch16-zero-shot)\n",
    "    frame_sample_rate = 1\n",
    "    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=seg_len)\n",
    "    video = vr.get_batch(indices).asnumpy()  # (clip_len, H, W, C)\n",
    "    inputs = processor(videos=list(video), return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    batch_size, num_frames, channels, height, width = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(-1, channels, height, width)  # [batch_size * num_frames, channels, height, width]\n",
    "    inputs['pixel_values'] = pixel_values.to('cuda')\n",
    "    with torch.no_grad():\n",
    "      outputs = video_model(**inputs)\n",
    "    frame_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    frame_embeddings = frame_embeddings.view(batch_size, num_frames, -1)\n",
    "    # average pooling over frame_embeddings\n",
    "    video_embedding = frame_embeddings.mean(dim=1) # (batch_size, emb_size) # надеюсь emb_size=1024 иначе еще линейный слой\n",
    "    video_embs.append(video_embedding)\n",
    "    video_embs_dict[video_path] = video_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "IFDLyEsV41Hn"
   },
   "outputs": [],
   "source": [
    "train_df['video_emb'] = [i.cpu().detach() for i in video_embs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbwrX68-41Hn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TCe9hGZw41Ho",
    "outputId": "52cb8fd2-6c47-4a48-c3d5-018f15cc7adf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title_description</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>video_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>9d37073c63008c784bf95a8a4d1715d5</td>\n",
       "      <td>Title: Punch Box. Серия 1. Пельмень vs Керам. ...</td>\n",
       "      <td>Первый кулак России, который сделан нокаут в т...</td>\n",
       "      <td>Массовая культура</td>\n",
       "      <td>[[tensor(-0.1769), tensor(0.2236), tensor(0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>cb8f4c48fd10af6768c3a89a5f9ab2f7</td>\n",
       "      <td>Title: Команда 3/21 в ГрандТуре «Байкальская м...</td>\n",
       "      <td>Погнали</td>\n",
       "      <td>Путешествия</td>\n",
       "      <td>[[tensor(-0.2604), tensor(-0.0189), tensor(0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              video_id  \\\n",
       "1237  9d37073c63008c784bf95a8a4d1715d5   \n",
       "1107  cb8f4c48fd10af6768c3a89a5f9ab2f7   \n",
       "\n",
       "                                      title_description  \\\n",
       "1237  Title: Punch Box. Серия 1. Пельмень vs Керам. ...   \n",
       "1107  Title: Команда 3/21 в ГрандТуре «Байкальская м...   \n",
       "\n",
       "                                             audio_text           tag_list  \\\n",
       "1237  Первый кулак России, который сделан нокаут в т...  Массовая культура   \n",
       "1107                                            Погнали        Путешествия   \n",
       "\n",
       "                                              video_emb  \n",
       "1237  [[tensor(-0.1769), tensor(0.2236), tensor(0.00...  \n",
       "1107  [[tensor(-0.2604), tensor(-0.0189), tensor(0.0...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Z2MPBlfNbVjn"
   },
   "outputs": [],
   "source": [
    "general_tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "WKGyos6V41Ho"
   },
   "outputs": [],
   "source": [
    "target_model = SentenceTransformer('intfloat/multilingual-e5-large-instruct').to('cuda')\n",
    "dim = 1024 # размерность эмбеддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "MYY8DM5c41Ho"
   },
   "outputs": [],
   "source": [
    "fusion_model = FusionEmbedModel(video_emb_size=768,\n",
    "                                text_emb_size=1024,\n",
    "                                audio_text_emb_size=1024,\n",
    "                                out_emb_size=1024, num_heads=8).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "hm0MZ8ebbPN1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 108\u001b[0m\n\u001b[1;32m    102\u001b[0m             total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtrain_triplet_loss_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfusion_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfusion_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtarget_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 66\u001b[0m, in \u001b[0;36mtrain_triplet_loss_model\u001b[0;34m(fusion_model, target_model, train_loader, fusion_optimizer, target_optimizer, epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m anchor_text_embs \u001b[38;5;241m=\u001b[39m text_model\u001b[38;5;241m.\u001b[39mencode(anchor_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m anchor_audio_embs \u001b[38;5;241m=\u001b[39m audio_model\u001b[38;5;241m.\u001b[39mencode(anchor_audio, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m anchor_video_embs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_video\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m anchor_embeddings \u001b[38;5;241m=\u001b[39m fusion_model(\n\u001b[1;32m     69\u001b[0m     description_title_emb\u001b[38;5;241m=\u001b[39manchor_text_embs,\n\u001b[1;32m     70\u001b[0m     audio_text_emb\u001b[38;5;241m=\u001b[39manchor_audio_embs,\n\u001b[1;32m     71\u001b[0m     video_emb\u001b[38;5;241m=\u001b[39manchor_video_embs\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m positive_text_embs \u001b[38;5;241m=\u001b[39m text_model\u001b[38;5;241m.\u001b[39mencode(positive_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "fusion_optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=3e-4)\n",
    "target_optimizer = torch.optim.AdamW(target_model.parameters(), lr=3e-4)\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.classes = dataframe['tag_list'].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_class = self.data.iloc[idx]['tag_list']\n",
    "\n",
    "        positive_text = self.data.iloc[idx]['title_description']\n",
    "        positive_audio = self.data.iloc[idx]['audio_text']\n",
    "        positive_video = self.data.iloc[idx]['video_emb']\n",
    "\n",
    "        positive_samples = self.data[(self.data['tag_list'] == anchor_class) & (self.data.index != idx)]\n",
    "        if len(positive_samples) > 0:\n",
    "            anchor_sample = positive_samples.sample(1).iloc[0]\n",
    "        else:\n",
    "            anchor_sample = self.data.iloc[idx]\n",
    "\n",
    "        anchor_text = anchor_sample['title_description']\n",
    "        anchor_audio = anchor_sample['audio_text']\n",
    "        anchor_video = anchor_sample['video_emb']\n",
    "\n",
    "        negative_samples = self.data[self.data['tag_list'] != anchor_class]\n",
    "        negative_sample = negative_samples.sample(1).iloc[0]\n",
    "        negative_text = negative_sample['title_description']\n",
    "        negative_audio = negative_sample['audio_text']\n",
    "        negative_video = negative_sample['video_emb']\n",
    "\n",
    "        return (\n",
    "            anchor_text, anchor_audio, anchor_video,\n",
    "            positive_text, positive_audio, positive_video,\n",
    "            negative_text, negative_audio, negative_video\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TripletDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    tokens = general_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    return {key: value.to('cuda') for key, value in tokens.items()}\n",
    "\n",
    "def train_triplet_loss_model(fusion_model, target_model, train_loader,\n",
    "                             fusion_optimizer, target_optimizer, epochs=2):\n",
    "    fusion_model.train()\n",
    "    target_model.train()\n",
    "    text_model.train()\n",
    "    audio_model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            (anchor_text, anchor_audio, anchor_video,\n",
    "             positive_text, positive_audio, positive_video,\n",
    "             negative_text, negative_audio, negative_video) = batch\n",
    "\n",
    "            anchor_text_embs = text_model.encode(anchor_text, convert_to_tensor=True).unsqueeze(1)\n",
    "            anchor_audio_embs = audio_model.encode(anchor_audio, convert_to_tensor=True).unsqueeze(1)\n",
    "            anchor_video_embs = torch.stack(anchor_video).to('cuda')\n",
    "\n",
    "            anchor_embeddings = fusion_model(\n",
    "                description_title_emb=anchor_text_embs,\n",
    "                audio_text_emb=anchor_audio_embs,\n",
    "                video_emb=anchor_video_embs\n",
    "            )\n",
    "\n",
    "            positive_text_embs = text_model.encode(positive_text, convert_to_tensor=True).unsqueeze(1)\n",
    "            positive_audio_embs = audio_model.encode(positive_audio, convert_to_tensor=True).unsqueeze(1)\n",
    "            positive_video_embs = torch.stack(positive_video).to('cuda')\n",
    "\n",
    "            positive_embeddings = fusion_model(\n",
    "                description_title_emb=positive_text_embs,\n",
    "                audio_text_emb=positive_audio_embs,\n",
    "                video_emb=positive_video_embs\n",
    "            )\n",
    "\n",
    "            negative_text_embs = text_model.encode(negative_text, convert_to_tensor=True).unsqueeze(1)\n",
    "            negative_audio_embs = audio_model.encode(negative_audio, convert_to_tensor=True).unsqueeze(1)\n",
    "            negative_video_embs = torch.stack(negative_video).to('cuda')\n",
    "\n",
    "            negative_embeddings = fusion_model(\n",
    "                description_title_emb=negative_text_embs,\n",
    "                audio_text_emb=negative_audio_embs,\n",
    "                video_emb=negative_video_embs\n",
    "            )\n",
    "\n",
    "            loss = triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "            fusion_optimizer.zero_grad()\n",
    "            target_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            fusion_optimizer.step()\n",
    "            target_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "\n",
    "\n",
    "train_triplet_loss_model(fusion_model=fusion_model, target_model=target_model,\n",
    "                         train_loader=train_loader, fusion_optimizer=fusion_optimizer,\n",
    "                         target_optimizer=target_optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0eMjYEt8o9v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "EnIt-SiG8pAb"
   },
   "outputs": [],
   "source": [
    "# METRICS\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def iou_metric(ground_truth, predictions):\n",
    "    iou =  len(set.intersection(set(ground_truth), set(predictions)))\n",
    "    iou = iou/(len(set(ground_truth).union(set(predictions))))\n",
    "    return iou\n",
    "\n",
    "def split_tags(tag_list):\n",
    "    final_tag_list = []\n",
    "    for tag in tag_list:\n",
    "        tags = tag.split(\": \")\n",
    "        if len(tags) == 3:\n",
    "            final_tag_list.append(tags[0])\n",
    "            final_tag_list.append(tags[0] + \": \" + tags[1])\n",
    "            final_tag_list.append(tags[0]+ \": \" + tags[1] + \": \" + tags[2])\n",
    "        elif len(tags) == 2:\n",
    "            final_tag_list.append(tags[0])\n",
    "            final_tag_list.append(tags[0] + \": \" + tags[1])\n",
    "        elif len(tags) == 1:\n",
    "            final_tag_list.append(tags[0])\n",
    "        else:\n",
    "            print(\"NOT IMPLEMENTED!!!!\", tag)\n",
    "    return final_tag_list\n",
    "\n",
    "def find_iou_for_sample_submission(pred_submission, true_submission):\n",
    "    ground_truth_df = true_submission\n",
    "    ground_truth_df[\"tag_list\"] = ground_truth_df[\"tag_list\"].str.split(', ')\n",
    "    ground_truth_df[\"tags_split\"] = ground_truth_df[\"tag_list\"].apply(lambda l: split_tags(l))\n",
    "\n",
    "    predictions_df = pred_submission\n",
    "#     predictions_df[\"predicted_tags\"] = predictions_df[\"predicted_tags\"].apply(ast.literal_eval\n",
    "    predictions_df[\"predicted_tags_split\"] = predictions_df[\"predicted_tags\"] = predictions_df[\"predicted_tags\"].apply(\n",
    "                                                                lambda l: split_tags(l) if not isinstance(l, float) else l\n",
    "                                                )\n",
    "\n",
    "    iou=0\n",
    "    counter = 0\n",
    "    for i, row in ground_truth_df.iterrows():\n",
    "        predicted_tags = predictions_df[predictions_df[\"video_id\"]==row[\"video_id\"]][\"predicted_tags_split\"].values[0]\n",
    "        iou_temp=iou_metric(row['tags_split'], predicted_tags)\n",
    "        iou+=iou_temp\n",
    "        counter+=1\n",
    "\n",
    "    return iou/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "CBeU4feL6PN-"
   },
   "outputs": [],
   "source": [
    "# INFERENCE (по test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "07SWaJ_iDgQ4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "video_model.eval()\n",
    "video_embs_dict_test = dict()\n",
    "video_embs_test = []\n",
    "for i in test_df.iterrows():\n",
    "  # видео эмбеддинги засовываем в test_df в новую колонку\n",
    "  video_path = \"/home/jovyan/lost+found/videos_2/\" + str(i[1].video_id) + \".mp4\" # поменять на директорию, где хранятся видео, типо video_id = \"folder/\" + video_id\n",
    "  if video_path in video_embs_dict_test:\n",
    "    video_embs_test.append(video_embs_dict_test[video_path])\n",
    "  else:\n",
    "    vr = VideoReader(video_path)\n",
    "    seg_len = len(vr)\n",
    "    clip_len = 32 # для конкретной модели (microsoft/xclip-base-patch16-zero-shot)\n",
    "    frame_sample_rate = 1\n",
    "    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=seg_len)\n",
    "    video = vr.get_batch(indices).asnumpy()  # (clip_len, H, W, C)\n",
    "    inputs = processor(videos=list(video), return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    batch_size, num_frames, channels, height, width = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(-1, channels, height, width)  # [batch_size * num_frames, channels, height, width]\n",
    "    inputs['pixel_values'] = pixel_values.to('cuda')\n",
    "    with torch.no_grad():\n",
    "      outputs = video_model(**inputs)\n",
    "    frame_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    frame_embeddings = frame_embeddings.view(batch_size, num_frames, -1)\n",
    "    # average pooling over frame_embeddings\n",
    "    video_embedding = frame_embeddings.mean(dim=1) # (batch_size, emb_size) # надеюсь emb_size=1024 иначе еще линейный слой\n",
    "    video_embs_test.append(video_embedding)\n",
    "    video_embs_dict_test[video_path] = video_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title_description</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>video_emb</th>\n",
       "      <th>title_description_vector</th>\n",
       "      <th>audio_text_vector</th>\n",
       "      <th>common_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>a40df9cd5c1517f478b458d74081ab12</td>\n",
       "      <td>Title: Один выходной | Сезон 2 | Выпуск 1 | Ск...</td>\n",
       "      <td>Можно все, вообще нет правила и в этом кайф. Т...</td>\n",
       "      <td>События и достопримечательности: Активный отдых</td>\n",
       "      <td>[[tensor(-0.2840), tensor(-0.0927), tensor(-0....</td>\n",
       "      <td>[0.02019478, 0.021183835, -0.028113348, -0.039...</td>\n",
       "      <td>[0.0122921225, -0.0072347517, -0.012975653, -0...</td>\n",
       "      <td>[-0.07921195, -0.041533127, -0.03266341, -0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>fbbd10e0e69381ef8d56896392eef543</td>\n",
       "      <td>Title: Тот Самый Мент I Выпуск 74 I Начальник ...</td>\n",
       "      <td>Присаживайтесь в Москве</td>\n",
       "      <td>Массовая культура: Юмор и сатира</td>\n",
       "      <td>[[tensor(-0.3112), tensor(0.0429), tensor(-0.0...</td>\n",
       "      <td>[0.025576632, 0.012932097, -0.018839719, -0.06...</td>\n",
       "      <td>[0.019080197, 0.01883682, -0.023289552, -0.045...</td>\n",
       "      <td>[-0.084199436, -0.07979075, -0.09679501, -0.11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              video_id  \\\n",
       "377   a40df9cd5c1517f478b458d74081ab12   \n",
       "1175  fbbd10e0e69381ef8d56896392eef543   \n",
       "\n",
       "                                      title_description  \\\n",
       "377   Title: Один выходной | Сезон 2 | Выпуск 1 | Ск...   \n",
       "1175  Title: Тот Самый Мент I Выпуск 74 I Начальник ...   \n",
       "\n",
       "                                             audio_text  \\\n",
       "377   Можно все, вообще нет правила и в этом кайф. Т...   \n",
       "1175                            Присаживайтесь в Москве   \n",
       "\n",
       "                                             tag_list  \\\n",
       "377   События и достопримечательности: Активный отдых   \n",
       "1175                 Массовая культура: Юмор и сатира   \n",
       "\n",
       "                                              video_emb  \\\n",
       "377   [[tensor(-0.2840), tensor(-0.0927), tensor(-0....   \n",
       "1175  [[tensor(-0.3112), tensor(0.0429), tensor(-0.0...   \n",
       "\n",
       "                               title_description_vector  \\\n",
       "377   [0.02019478, 0.021183835, -0.028113348, -0.039...   \n",
       "1175  [0.025576632, 0.012932097, -0.018839719, -0.06...   \n",
       "\n",
       "                                      audio_text_vector  \\\n",
       "377   [0.0122921225, -0.0072347517, -0.012975653, -0...   \n",
       "1175  [0.019080197, 0.01883682, -0.023289552, -0.045...   \n",
       "\n",
       "                                          common_vector  \n",
       "377   [-0.07921195, -0.041533127, -0.03266341, -0.11...  \n",
       "1175  [-0.084199436, -0.07979075, -0.09679501, -0.11...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ldRIwOSUDgQ4"
   },
   "outputs": [],
   "source": [
    "test_df['video_emb'] = [i.cpu().detach() for i in video_embs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "B1CeDwJ7Evsa"
   },
   "outputs": [],
   "source": [
    "test_df['title_description_vector'] = test_df['title_description'].apply(lambda l: text_model.encode(l, convert_to_tensor=True).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "l9XXIsb8Evsa"
   },
   "outputs": [],
   "source": [
    "test_df['audio_text_vector'] = test_df['audio_text'].apply(lambda l: audio_model.encode(l, convert_to_tensor=True).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19125/1084964683.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  video_emb = torch.tensor(l['video_emb']).unsqueeze(0).to('cuda')  # (1, video_emb_dim)\n"
     ]
    }
   ],
   "source": [
    "def compute_common_vector(l):\n",
    "    description_title_emb = torch.tensor(l['title_description_vector']).unsqueeze(0).unsqueeze(1).to('cuda')  # (1, 1, emb_dim)\n",
    "    audio_text_emb = torch.tensor(l['audio_text_vector']).unsqueeze(0).unsqueeze(1).to('cuda')  # (1, 1, emb_dim)\n",
    "    video_emb = torch.tensor(l['video_emb']).unsqueeze(0).to('cuda')  # (1, video_emb_dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        common_vector = fusion_model(\n",
    "            description_title_emb=description_title_emb,\n",
    "            audio_text_emb=audio_text_emb,\n",
    "            video_emb=video_emb\n",
    "        )  # (1, output_dim)\n",
    "\n",
    "    return common_vector.cpu().numpy()[0]\n",
    "\n",
    "test_df['common_vector'] = test_df.apply(compute_common_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "611it [00:18, 32.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_tags():\n",
    "    target_model.eval()\n",
    "    tags = {}\n",
    "    with torch.no_grad():\n",
    "        for i, row in tqdm(taxonomy.iterrows()):\n",
    "            if isinstance(row['Уровень 1 (iab)'], str):\n",
    "                tags[row['Уровень 1 (iab)']] = target_model.encode(row['Уровень 1 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "            if isinstance(row['Уровень 2 (iab)'], str):\n",
    "                tags[row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']] = target_model.encode(row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "            if isinstance(row['Уровень 3 (iab)'], str):\n",
    "                tags[row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']+\": \"+row['Уровень 3 (iab)']] = target_model.encode(row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']+\": \"+row['Уровень 3 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "        return tags\n",
    "\n",
    "tags = get_tags()\n",
    "tags_list = list(tags.keys())\n",
    "vectors = np.array(list(tags.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "610\n"
     ]
    }
   ],
   "source": [
    "index = faiss.index_factory(dim, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "print(index.ntotal)\n",
    "index.add(vectors)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title_description</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>video_emb</th>\n",
       "      <th>title_description_vector</th>\n",
       "      <th>audio_text_vector</th>\n",
       "      <th>common_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>8a762b2f22accc32159926c8054fd8e9</td>\n",
       "      <td>Title: location | Pompeya - Odelay. Descriptio...</td>\n",
       "      <td>Маленький</td>\n",
       "      <td>Музыка и аудио: Разное (Музыка и аудио)</td>\n",
       "      <td>[[tensor(-0.2467), tensor(0.1080), tensor(-0.0...</td>\n",
       "      <td>[0.010618988, 0.018526142, -0.019410837, -0.04...</td>\n",
       "      <td>[-0.0023610704, 0.03212811, -0.014340383, -0.0...</td>\n",
       "      <td>[-0.09672257, -0.027643707, -0.037916858, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>78002eb306ae22e44147464bc45842f8</td>\n",
       "      <td>Title: Много половин. Акмаль и его девушка Саш...</td>\n",
       "      <td>Сложно не заметить легендарную кредитную карту...</td>\n",
       "      <td>Карьера</td>\n",
       "      <td>[[tensor(-0.6046), tensor(-0.0152), tensor(0.0...</td>\n",
       "      <td>[0.013182346, 0.016781544, -0.025050532, -0.02...</td>\n",
       "      <td>[0.011375706, 0.017268492, -0.037079215, -0.04...</td>\n",
       "      <td>[-0.07911661, -0.06265162, -0.11458967, -0.096...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              video_id  \\\n",
       "1030  8a762b2f22accc32159926c8054fd8e9   \n",
       "819   78002eb306ae22e44147464bc45842f8   \n",
       "\n",
       "                                      title_description  \\\n",
       "1030  Title: location | Pompeya - Odelay. Descriptio...   \n",
       "819   Title: Много половин. Акмаль и его девушка Саш...   \n",
       "\n",
       "                                             audio_text  \\\n",
       "1030                                          Маленький   \n",
       "819   Сложно не заметить легендарную кредитную карту...   \n",
       "\n",
       "                                     tag_list  \\\n",
       "1030  Музыка и аудио: Разное (Музыка и аудио)   \n",
       "819                                   Карьера   \n",
       "\n",
       "                                              video_emb  \\\n",
       "1030  [[tensor(-0.2467), tensor(0.1080), tensor(-0.0...   \n",
       "819   [[tensor(-0.6046), tensor(-0.0152), tensor(0.0...   \n",
       "\n",
       "                               title_description_vector  \\\n",
       "1030  [0.010618988, 0.018526142, -0.019410837, -0.04...   \n",
       "819   [0.013182346, 0.016781544, -0.025050532, -0.02...   \n",
       "\n",
       "                                      audio_text_vector  \\\n",
       "1030  [-0.0023610704, 0.03212811, -0.014340383, -0.0...   \n",
       "819   [0.011375706, 0.017268492, -0.037079215, -0.04...   \n",
       "\n",
       "                                          common_vector  \n",
       "1030  [-0.09672257, -0.027643707, -0.037916858, -0.0...  \n",
       "819   [-0.07911661, -0.06265162, -0.11458967, -0.096...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>predicted_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>49c3735a65a729ac595ef2d5e00cc473</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>476ff4965bb738733c12ac044ea155d3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             video_id predicted_tags\n",
       "622  49c3735a65a729ac595ef2d5e00cc473            NaN\n",
       "479  476ff4965bb738733c12ac044ea155d3            NaN"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005889281507656065"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn = 1\n",
    "\n",
    "sample_submission = pd.DataFrame(data=data['video_id'].to_list(), columns=['video_id'])\n",
    "sample_submission['predicted_tags'] = np.nan\n",
    "sample_submission['predicted_tags'] = sample_submission['predicted_tags'].astype('object')\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "#   topn = int(model_num_tags.predict([row['common_vector']])[0])\n",
    "  \n",
    "  scores, predictions = index.search(np.array([row['common_vector']]), topn)\n",
    "  index_i = sample_submission[sample_submission.video_id == row.video_id].index\n",
    "  sample_submission.at[index_i[0], 'predicted_tags'] = [\n",
    "      tags_list[tag] for i, tag in enumerate(predictions[0])\n",
    "  ]\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "find_iou_for_sample_submission(sample_submission, test_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woRHe4wdFIPK"
   },
   "outputs": [],
   "source": [
    "def get_tags():\n",
    "    tags = {}\n",
    "    for i, row in tqdm(taxonomy.iterrows()):\n",
    "        if isinstance(row['Уровень 1 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)']] = model.encode(row['Уровень 1 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "        if isinstance(row['Уровень 2 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']] = model.encode(row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "        if isinstance(row['Уровень 3 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']+\": \"+row['Уровень 3 (iab)']] = model.encode(row['Уровень 1 (iab)']+ \": \"+row['Уровень 2 (iab)']+\": \"+row['Уровень 3 (iab)'], convert_to_tensor=True).cpu().numpy()#.tolist()\n",
    "    return tags\n",
    "\n",
    "tags = get_tags()\n",
    "tags_list = list(tags.keys())\n",
    "vectors = np.array(list(tags.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_xCgHSiFIRv"
   },
   "outputs": [],
   "source": [
    "index = faiss.index_factory(dim, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "print(index.ntotal)\n",
    "index.add(vectors)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hmMNNVuBvm-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5ovCZcjBl8L"
   },
   "outputs": [],
   "source": [
    "topn = 1\n",
    "\n",
    "sample_submission = pd.DataFrame(data=test_df['video_id'].to_list(), columns=['video_id'])\n",
    "sample_submission['predicted_tags'] = np.nan\n",
    "sample_submission['predicted_tags'] = sample_submission['predicted_tags'].astype('object')\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "#   topn = int(model_num_tags.predict([row['common_vector']])[0])\n",
    "  scores, predictions = index.search(np.array([row['common_vector']]), topn)\n",
    "  index_i = sample_submission[sample_submission.video_id == row.video_id].index\n",
    "  sample_submission.at[index_i[0], 'predicted_tags'] = [\n",
    "      tags_list[tag] for i, tag in enumerate(predictions[0])\n",
    "  ]\n",
    "\n",
    "data_copy = data.copy()\n",
    "find_iou_for_sample_submission(sample_submission, data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1leOBs-tBYzF"
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"sample_submission.csv\", index_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fupNnO-cBY11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA81bxMrbPYR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLncaIwfbPdu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQZjY9xtbPg5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
